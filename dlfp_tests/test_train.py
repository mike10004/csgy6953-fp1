#!/usr/bin/env python3

from unittest import TestCase
import numpy as np
import torch
import torch.random
from torch import Tensor
from torch.utils.data import DataLoader

from dlfp.models import Seq2SeqTransformer
from dlfp.models import create_model
from dlfp.train import Trainer
from dlfp.train import TrainLoaders
from dlfp.common import noop
import dlfp_tests.tools

dlfp_tests.tools.suppress_cuda_warning()


class TrainerTest(TestCase):

    verbose: bool = False

    def test_create_mask(self):
        train_dataset = dlfp_tests.tools.load_multi30k_dataset(split='train')
        bilinguist = dlfp_tests.tools.get_multi30k_de_en_bilinguist()
        dataloader = DataLoader(train_dataset, batch_size=16, collate_fn=bilinguist.collate)
        src, tgt = next(iter(dataloader))
        tgt_input = tgt[:-1, :]
        masks = Trainer.create_mask_static(src, tgt_input, device="cpu", pad_idx=bilinguist.source.specials.indexes.pad)
        self.assertSetEqual({torch.bool}, set(mask.dtype for mask in masks))


    def test_train(self):
        with torch.random.fork_rng():
            torch.random.manual_seed(0)
            bilinguist = dlfp_tests.tools.get_multi30k_de_en_bilinguist()
            batch_size = 16
            train_dataset = dlfp_tests.tools.load_multi30k_dataset(split='train')
            valid_dataset = dlfp_tests.tools.load_multi30k_dataset(split='valid')
            train_dataset = dlfp_tests.tools.truncate_dataset(train_dataset, size=200)
            valid_dataset = dlfp_tests.tools.truncate_dataset(valid_dataset, size=64)
            device = dlfp_tests.tools.get_device()
            model = create_model(
                src_vocab_size=len(bilinguist.source.vocab),
                tgt_vocab_size=len(bilinguist.target.vocab),
            ).to(device)
            trainer = Trainer(model, pad_idx=bilinguist.source.specials.indexes.pad, device=device)
            trainer.hide_progress = not self.verbose
            callback = noop if not self.verbose else None
            loaders = TrainLoaders.from_datasets(train_dataset, valid_dataset, batch_size=batch_size, collate_fn=bilinguist.collate)
            epoch_count = 1
            results = trainer.train(loaders, epoch_count, callback=callback)
            self.assertEqual(epoch_count, len(results))
            weights = get_weight_sample(model, trainer, next(iter(loaders.valid)))
            # print(weights.shape)
            print(weights.shape)
            # import json
            # print(json.dumps(weights[0].detach().cpu().numpy().tolist()))
            expected = [[0.10272958874702454, 0.11018285155296326, 0.13172799348831177, 0.12050598859786987, 0.08412271738052368, 0.07200094312429428, 0.06687989085912704, 0.07065194845199585, 0.07974986732006073, 0.08400049805641174, 0.07744771242141724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10370934754610062, 0.1128590926527977, 0.12818686664104462, 0.12100747972726822, 0.09013029932975769, 0.07594470679759979, 0.06946083903312683, 0.06959850341081619, 0.07643020153045654, 0.07680484652519226, 0.07586784660816193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11374163627624512, 0.11563783884048462, 0.11263643205165863, 0.10212966054677963, 0.08732501417398453, 0.07544257491827011, 0.07203555107116699, 0.07810153812170029, 0.08172710984945297, 0.08846218883991241, 0.0727604329586029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10618321597576141, 0.10260416567325592, 0.08823049068450928, 0.09505316615104675, 0.08830947428941727, 0.07767850905656815, 0.07660673558712006, 0.09153151512145996, 0.09359110891819, 0.09749011695384979, 0.08272150158882141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09755994379520416, 0.10726335644721985, 0.10045026987791061, 0.09605943411588669, 0.0963280126452446, 0.07835398614406586, 0.07272129505872726, 0.0853436067700386, 0.08955737203359604, 0.08963170647621155, 0.08673103153705597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10246696323156357, 0.1125873476266861, 0.08648829907178879, 0.09355086088180542, 0.09027048945426941, 0.08322037756443024, 0.08706510812044144, 0.087338387966156, 0.09280500560998917, 0.08530409634113312, 0.07890306413173676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10828495770692825, 0.12006274610757828, 0.10275521129369736, 0.09479369223117828, 0.08040150254964828, 0.08235152065753937, 0.07823817431926727, 0.08466759324073792, 0.0860055610537529, 0.08648323267698288, 0.0759558230638504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1171732097864151, 0.11654821783304214, 0.10495161265134811, 0.0984780415892601, 0.08380816131830215, 0.08028487861156464, 0.07727029919624329, 0.07836230099201202, 0.08494774997234344, 0.08333297818899155, 0.07484256476163864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11512637883424759, 0.12246423959732056, 0.11876311153173447, 0.1088692918419838, 0.08654177188873291, 0.07957574725151062, 0.07122190296649933, 0.07208945602178574, 0.07558784633874893, 0.07740321010351181, 0.07235702872276306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11057329922914505, 0.13138078153133392, 0.11519404500722885, 0.11315886676311493, 0.08728207647800446, 0.08195789158344269, 0.075215645134449, 0.07216398417949677, 0.07321417331695557, 0.07245739549398422, 0.06740179657936096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11140000820159912, 0.12305775284767151, 0.10865531861782074, 0.10561616718769073, 0.08817987889051437, 0.08044946938753128, 0.077811598777771, 0.07302650809288025, 0.07717294245958328, 0.07753029465675354, 0.07710006833076477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10717373341321945, 0.1258828490972519, 0.10332842171192169, 0.10571861267089844, 0.08861306309700012, 0.07344319671392441, 0.07793610543012619, 0.07244759798049927, 0.07670235633850098, 0.08517574518918991, 0.08357833325862885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10361288487911224, 0.12336517870426178, 0.10286762565374374, 0.10002274811267853, 0.0854281634092331, 0.07218571752309799, 0.07835522294044495, 0.07178996503353119, 0.07850529998540878, 0.09278786182403564, 0.09107932448387146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10229441523551941, 0.12032264471054077, 0.10270142555236816, 0.09767180681228638, 0.08394787460565567, 0.07161486148834229, 0.07836276292800903, 0.07235054671764374, 0.07993074506521225, 0.09711826592683792, 0.09368463605642319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10394501686096191, 0.11786232143640518, 0.10199940204620361, 0.09912633895874023, 0.08581076562404633, 0.0719221830368042, 0.07820754498243332, 0.0742705911397934, 0.0804019346833229, 0.09648953378200531, 0.089964359998703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10753605514764786, 0.1171417236328125, 0.101290762424469, 0.10263874381780624, 0.08948700875043869, 0.07282846421003342, 0.07756523042917252, 0.07588116079568863, 0.07942282408475876, 0.0931105762720108, 0.0830974355340004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11098968237638474, 0.1187564879655838, 0.10211019217967987, 0.10539577901363373, 0.09198372066020966, 0.07357241213321686, 0.07612748444080353, 0.07570852339267731, 0.07757126539945602, 0.09026728570461273, 0.07751717418432236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1130046471953392, 0.12138520926237106, 0.10512508451938629, 0.10551594197750092, 0.0916263610124588, 0.07394382357597351, 0.07431154698133469, 0.07374688237905502, 0.07610077410936356, 0.08955519646406174, 0.07568448781967163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11271598935127258, 0.12204992771148682, 0.10892841964960098, 0.10360537469387054, 0.08893221616744995, 0.0747639387845993, 0.07341980934143066, 0.0715545117855072, 0.07558801770210266, 0.09104115515947342, 0.07740066200494766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10949935764074326, 0.11905565857887268, 0.11118295788764954, 0.10238946229219437, 0.08592480421066284, 0.07658995687961578, 0.07435739040374756, 0.07136114686727524, 0.07589804381132126, 0.0935639813542366, 0.0801772028207779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10444552451372147, 0.11350775510072708, 0.11068621277809143, 0.10413634032011032, 0.08506336063146591, 0.07895801216363907, 0.0765361487865448, 0.07401788979768753, 0.07675640285015106, 0.09465343505144119, 0.08123894035816193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09978827834129333, 0.10782463103532791, 0.10856406390666962, 0.10888843983411789, 0.08729493618011475, 0.0810314416885376, 0.0785302147269249, 0.07778322696685791, 0.07787704467773438, 0.09240102767944336, 0.08001662790775299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0969681367278099, 0.10442034900188446, 0.10750766098499298, 0.1149304136633873, 0.09112650156021118, 0.08195627480745316, 0.07910250127315521, 0.07970646023750305, 0.07886835187673569, 0.08765006065368652, 0.07776327431201935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09646464884281158, 0.10489502549171448, 0.10958554595708847, 0.11939353495836258, 0.09406501799821854, 0.08147795498371124, 0.07761147618293762, 0.07818663865327835, 0.07954273372888565, 0.08306723833084106, 0.07571018487215042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09783002734184265, 0.10866682231426239, 0.1142568364739418, 0.11975506693124771, 0.0945785865187645, 0.08052724599838257, 0.0747017189860344, 0.07399608939886093, 0.07996724545955658, 0.08084981143474579, 0.07487054914236069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09944070130586624, 0.11307629197835922, 0.11843044310808182, 0.11621715128421783, 0.09301958233118057, 0.08025620877742767, 0.07209530472755432, 0.06969700008630753, 0.0800432562828064, 0.08175314962863922, 0.07597091794013977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09964176267385483, 0.11541131883859634, 0.11895199865102768, 0.11181871592998505, 0.09129952639341354, 0.08081598579883575, 0.07101915776729584, 0.06795097887516022, 0.07975701242685318, 0.08492227643728256, 0.07841126620769501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09883861243724823, 0.11491452157497406, 0.11507301032543182, 0.10979747772216797, 0.09137631952762604, 0.08137676119804382, 0.07183915376663208, 0.06948278844356537, 0.07956700772047043, 0.08776579052209854, 0.07996854186058044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09899654239416122, 0.11320653557777405, 0.10888059437274933, 0.11125840991735458, 0.09345261007547379, 0.08105982840061188, 0.07470015436410904, 0.0726652666926384, 0.07977163046598434, 0.08748670667409897, 0.0785217136144638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10086933523416519, 0.11280260980129242, 0.10378917306661606, 0.11536768078804016, 0.09575889259576797, 0.07978250831365585, 0.07883273810148239, 0.07510408759117126, 0.07967270165681839, 0.08377672731876373, 0.07424356788396835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
            expected = np.array(expected, dtype=np.float32).reshape(30, 30)
            weight_slice = weights[0].detach().cpu().numpy()
            np.testing.assert_array_almost_equal(expected, weight_slice, decimal=4)


def get_weight_sample(model: Seq2SeqTransformer, trainer: Trainer, tensor_pair) -> Tensor:
    model.eval()
    source, target = tensor_pair
    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = trainer.create_mask(source, target)
    src_emb = model.positional_encoding(model.src_tok_emb(source))
    encoderlayer = model.transformer.encoder.layers[0]
    x = encoderlayer.norm1(src_emb)
    output, weights = encoderlayer.self_attn(x, x, x, attn_mask = src_mask, key_padding_mask=src_padding_mask)
    return weights